# Coach Up AI API (.env.example)
# Copy to .env and fill with real values or set in your host

# Server
PORT=8001
LOG_LEVEL=info

# Convex backend (for secure session->userId and tracked skills lookups)
# Example: http://127.0.0.1:3210
CONVEX_URL=
# Optional HTTP timeout (seconds) for Convex requests; defaults to 10 if unset
CONVEX_TIMEOUT_SECONDS=

# Auth (Core -> AI)
SERVICE_SHARED_SECRET=
# or Clerk verification
CLERK_ISSUER_URL=
CLERK_AUDIENCE=

# Providers (placeholders / legacy)
OPENAI_API_KEY=
ANTHROPIC_API_KEY=
GEMINI_API_KEY=

# AI Provider selection (SPR-005)
# Global default provider; per-role overrides take precedence
# Supported: mock | google | openrouter
AI_PROVIDER=mock
AI_PROVIDER_CHAT=
AI_PROVIDER_CLASSIFIER=
AI_PROVIDER_ASSESS=

# Notes:
# - Per-role provider vars override AI_PROVIDER
# - Model vars below are optional; providers may apply a default

# Chat enable + model selection
AI_CHAT_ENABLED=0
# Example models:
#  - google: gemini-1.5-pro, gemini-1.5-flash
#  - openrouter: openai/gpt-4o-mini, anthropic/claude-3.5-sonnet
AI_CHAT_MODEL=

# Chat context
# Number of recent messages to include as context for chat responses.
# Clamped to [1,200]. Default 10 when unset/invalid.
CHAT_CONTEXT_LIMIT=10
# When 1, fetch chat context from Convex interactions; otherwise use in-memory transcripts.
CHAT_CONTEXT_FROM_CONVEX=0

# Classifier enable + model selection
# When AI_CLASSIFIER_ENABLED=1, /messages/ingest will use provider classifier via factory.
# If provider errors, it falls back to mock classifier, then to local heuristic.
AI_CLASSIFIER_ENABLED=0
# Example models mirror provider options above
AI_CLASSIFIER_MODEL=

# Assessment enable + model selection (placeholder for upcoming integration)
AI_ASSESS_ENABLED=0
AI_ASSESS_MODEL=

# Assessment output (SPR-005)
# The backend now persists assessments in the v2 skill-aligned payload ONLY.
# Optional salt used when hashing tracked skill IDs for logs/metrics/persistence
SKILL_HASH_SALT=

# Provider API keys
GOOGLE_API_KEY=
OPENROUTER_API_KEY=

# Observability (optional)
OTEL_EXPORTER_OTLP_ENDPOINT=

# Classifier & Worker (SPR-002)
# Confidence thresholds
CLASSIFIER_CONF_ACCEPT=0.7
CLASSIFIER_CONF_LOW=0.4
# Legacy flag to disable classifier fully (overrides AI_CLASSIFIER_ENABLED)
DISABLE_CLASSIFIER=0
# Worker/runtime tuning
WORKER_CONCURRENCY=2
ASSESSMENTS_MAX_RETRIES=3
ASSESSMENTS_BACKOFF_BASE_MS=200
ASSESSMENT_COOLDOWN_MS=10000
# Timeouts (ms)
ASSESS_PER_SKILL_TIMEOUT_MS=8000
ASSESS_GROUP_TIMEOUT_MS=15000

# Durable Queue (SQS FIFO) â€” Feature flag and config
# Enable SQS-backed queue instead of in-memory
USE_SQS=0
# Required when USE_SQS=1
AWS_REGION=us-east-1
# Example FIFO queue URL (AWS or LocalStack). Must end with .fifo for FIFO queues
# AWS_SQS_QUEUE_URL=https://sqs.us-east-1.amazonaws.com/123456789012/coach-up-assessments.fifo
AWS_SQS_QUEUE_URL=
# For local development with LocalStack
# e.g. http://localhost:4566
AWS_ENDPOINT_URL_SQS=
# Credentials (needed for AWS or LocalStack)
AWS_ACCESS_KEY_ID=test
AWS_SECRET_ACCESS_KEY=test

# Optional persistence hook (server-to-server HTTP callback)
# If set, the background worker will POST latest summaries to this URL.
# Example (Next.js UI route that writes to Convex):
#   PERSIST_ASSESSMENTS_URL=http://localhost:3100/api/assessments/convex/finalize
# Notes:
# - In mock mode on the UI (MOCK_CONVEX=1), finalize route skips Authorization to avoid 401s in tests.
# - For real Convex, set MOCK_CONVEX=0 on the UI and provide CONVEX_URL/NEXT_PUBLIC_CONVEX_URL.
# - If auth is enforced, set PERSIST_ASSESSMENTS_SECRET here and pass the same value to the UI server and tests.
PERSIST_ASSESSMENTS_URL=
# Optional bearer for simple auth (matched by the UI route if configured)
PERSIST_ASSESSMENTS_SECRET=
